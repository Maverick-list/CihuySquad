# GreenVision AI - Environment Configuration
# Copy this file to .env and update values as needed

# ============================================
# SERVER CONFIGURATION
# ============================================

# Server port
PORT=3000

# Node environment (development|production)
NODE_ENV=development

# ============================================
# OLLAMA CONFIGURATION
# ============================================

# Ollama server URL (local LLM runtime)
# Default: http://localhost:11434
# This is where Ollama LLM server is running
OLLAMA_URL=http://localhost:11434

# Ollama model name to use
# Options: mistral, neural-chat, dolphin-2.5, openchat, llama2
# Recommended: mistral (7B, balanced performance/quality)
OLLAMA_MODEL=mistral

# ============================================
# FRONTEND CONFIGURATION
# ============================================

# Frontend API base URL
# For local development
FRONTEND_API_URL=http://localhost:3000

# For production, change to your domain
# FRONTEND_API_URL=https://api.greenvision-ai.com

# ============================================
# LOGGING
# ============================================

# Log level (debug|info|warn|error)
LOG_LEVEL=info

# Enable request logging
LOG_REQUESTS=true

# ============================================
# CORS CONFIGURATION
# ============================================

# Allowed origins for CORS (comma-separated)
# Default: localhost:8000, localhost:8001, localhost:8002
ALLOWED_ORIGINS=http://localhost:8000,http://localhost:8001,http://localhost:8002,http://127.0.0.1:8000

# For production
# ALLOWED_ORIGINS=https://yourdomain.com,https://www.yourdomain.com

# ============================================
# NOTES
# ============================================

# 1. OLLAMA INSTALLATION
#    - Download from https://ollama.ai
#    - Run: ollama serve
#    - Pull model: ollama pull mistral
#    - Test: curl http://localhost:11434/api/tags

# 2. LOCAL DEVELOPMENT
#    - Frontend: python3 -m http.server 8000 (in public/ folder)
#    - Backend: npm start (in root folder)
#    - Access: http://localhost:8000

# 3. PRODUCTION DEPLOYMENT
#    - Set NODE_ENV=production
#    - Set OLLAMA_URL to production Ollama instance
#    - Update ALLOWED_ORIGINS to your domain
#    - Use environment variables, not this file
#    - Enable HTTPS/TLS

# 4. SECURITY
#    - Never commit .env file
#    - Keep secrets in environment variables only
#    - Use .env.example for template (no secrets)
#    - Rotate credentials regularly
